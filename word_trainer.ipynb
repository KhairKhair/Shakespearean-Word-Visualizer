{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import string\n",
    "import lightning as Li\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "from dash import Dash, dcc, html, Input, Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload dataset\n",
    "plays = pd.read_csv(\"datasets/shakespeare_plays.csv\")\n",
    "\n",
    "comedy_plays = plays[plays[\"genre\"] == \"Comedy\"]\n",
    "tragedy_plays = plays[plays[\"genre\"] == \"Tragedy\"]\n",
    "history_plays = plays[plays[\"genre\"] == \"History\"]\n",
    "\n",
    "comedy_text = comedy_plays[\"text\"].unique()\n",
    "tragedy_text = tragedy_plays[\"text\"].unique()\n",
    "history_text = history_plays[\"text\"].unique()\n",
    "\n",
    "#hyperparameter controlling context window\n",
    "L = 2\n",
    "#hyperparamter controlling neg/pos words ratio\n",
    "k = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27604\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# helper function to remove punctionation\n",
    "# makes words lowercase\n",
    "def clean_word(word):\n",
    "    word = word.lower()\n",
    "        \n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_word = word.translate(translator)\n",
    "        \n",
    "    return cleaned_word\n",
    "\n",
    "# first, create dictionary to store index of each word\n",
    "# will do the same for all texts to have same \"subspace\"\n",
    "\n",
    "#makes array of words\n",
    "comedy_words = np.array([clean_word(word) for string in comedy_text for word in string.split()])\n",
    "tragedy_words = np.array([clean_word(word) for string in tragedy_text for word in string.split()])\n",
    "history_words = np.array([clean_word(word) for string in history_text for word in string.split()])\n",
    "\n",
    "all_words = np.concatenate((comedy_words, tragedy_words, history_words))\n",
    "all_words = np.unique(all_words)\n",
    "np.save(\"all_words.npy\", all_words)\n",
    "\n",
    "#create described dict\n",
    "current_index = 0\n",
    "common_dict = {all_words[i]:i for i in range(len(all_words))}\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create (word, label) pairs\n",
    "def create_pairs(text, dict, L):\n",
    "    word_pair = {i:[] for i in range(len(dict))}\n",
    "    for sentence in text:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words)):\n",
    "            word = clean_word(words[i])\n",
    "            for j in range(1,L+1):\n",
    "                if i+j < len(words):\n",
    "                    context = clean_word(words[i+j])\n",
    "                    word_pair[dict[word]].append(dict[context])\n",
    "                if i-j >= 0:\n",
    "                    context = clean_word(words[i-j])\n",
    "                    word_pair[dict[word]].append(dict[context])\n",
    "    return word_pair\n",
    "\n",
    "\n",
    "comedy_pairs  = create_pairs(comedy_text, common_dict, L)\n",
    "tragedy_pairs  = create_pairs(tragedy_text, common_dict, L)\n",
    "history_pairs  = create_pairs(history_text, common_dict, L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayDataset(Dataset):\n",
    "    def __init__(self, word_pair, text, dict) -> None:\n",
    "        # creating final dataset\n",
    "        # adding negative words\n",
    "        self.words, self.pos, self.neg = [], [], []\n",
    "        vocab_size = len(dict)\n",
    "        for word, contexts in word_pair.items():\n",
    "            for pos in contexts:\n",
    "                neg = []\n",
    "                while len(neg) < k:\n",
    "                    temp = np.random.randint(vocab_size)\n",
    "                    if temp != word and temp not in contexts:\n",
    "                        neg.append(temp)\n",
    "                self.words.append(word)\n",
    "                self.pos.append(pos)\n",
    "                self.neg.append(neg)\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.pos[idx], self.neg[idx]\n",
    "        super().__init__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.words[idx], self.pos[idx], self.neg[idx]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "comedy_dataset = PlayDataset(comedy_pairs, comedy_text, common_dict)\n",
    "comedy_loader = DataLoader(comedy_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "tragedy_dataset = PlayDataset(tragedy_pairs, tragedy_text, common_dict)\n",
    "tragedy_loader = DataLoader(tragedy_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "history_dataset = PlayDataset(history_pairs, history_text, common_dict)\n",
    "history_loader = DataLoader(history_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class neural(Li.LightningModule):\n",
    "    def __init__(self, vocab_size, word_dim, k) -> None:\n",
    "        super().__init__()\n",
    "        self.w_embedding = nn.Embedding(vocab_size, word_dim, device='cpu')\n",
    "        self.c_embedding = nn.Embedding(vocab_size, word_dim, device='cpu')\n",
    "        # for every positive context word, we will have k negative ones.\n",
    "        self.k = k\n",
    "        self.learning_rate = 0.01\n",
    "\n",
    "\n",
    "    def forward(self, word, pos, negatives):\n",
    "        first_term = torch.mm(self.c_embedding(pos), self.w_embedding(word).T).sum()\n",
    "        first_term = (nn.functional.logsigmoid(first_term))\n",
    "\n",
    "\n",
    "        second_term = 0\n",
    "        for neg in negatives:\n",
    "            neg = torch.tensor(neg, device=self.device)\n",
    "            temp = torch.mm(-self.c_embedding(neg), self.w_embedding(word).T).sum()\n",
    "            temp = nn.functional.logsigmoid(temp)\n",
    "            second_term += temp\n",
    "\n",
    "        loss = -1 * (first_term + second_term)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        word, pos, neg = batch\n",
    "        word, pos = torch.tensor(word, device=self.device), torch.tensor(pos, device=self.device)\n",
    "        loss = self.forward(word, pos, neg) \n",
    "        self.log(\"loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamadaltrabulsi/Desktop/coding_2024/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/34657 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/2rdxqvg92lg7xvg6y302f5pr0000gn/T/ipykernel_61257/2220355775.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  word, pos = torch.tensor(word, device=self.device), torch.tensor(pos, device=self.device)\n",
      "/var/folders/xh/2rdxqvg92lg7xvg6y302f5pr0000gn/T/ipykernel_61257/2220355775.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  neg = torch.tensor(neg, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 34657/34657 [04:14<00:00, 136.10it/s, v_num=66, loss=-0.00]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 34657/34657 [04:14<00:00, 136.09it/s, v_num=66, loss=-0.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 22510/22510 [02:39<00:00, 141.55it/s, v_num=67, loss=-0.00]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 22510/22510 [02:39<00:00, 141.54it/s, v_num=67, loss=-0.00]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 23910/23910 [02:44<00:00, 145.53it/s, v_num=68, loss=0.0369]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 23910/23910 [02:44<00:00, 145.52it/s, v_num=68, loss=0.0369]\n"
     ]
    }
   ],
   "source": [
    "c_n = neural(len(common_dict),L,k)\n",
    "trainer = Li.Trainer(max_epochs=2, enable_model_summary=0)\n",
    "trainer.fit(c_n, train_dataloaders=comedy_loader)\n",
    "\n",
    "t_n = neural(len(common_dict),L,k)\n",
    "trainer = Li.Trainer(max_epochs=2, enable_model_summary=0)\n",
    "trainer.fit(t_n, train_dataloaders=tragedy_loader)\n",
    "\n",
    "h_n = neural(len(common_dict),L,k)\n",
    "trainer = Li.Trainer(max_epochs=2, enable_model_summary=0)\n",
    "trainer.fit(h_n, train_dataloaders=history_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the common words between the texts\n",
    "\n",
    "common_words = np.intersect1d(np.intersect1d(comedy_words, tragedy_words),history_words)\n",
    "common_words = np.array([clean_word(word) for word in common_words])\n",
    "common_words = common_words[common_words != \"\"]\n",
    "\n",
    "# sorting the words by how much they occur in all of the texts\n",
    "word_counts = Counter(common_words)\n",
    "common_words = sorted(word_counts.keys(), key=lambda word: word_counts[word], reverse=True)\n",
    "common_words = np.array(common_words)\n",
    "\n",
    "np.save(\"common_words.npy\", common_words)\n",
    "#words that are banned because they are not very interesting\n",
    "banned_words = ['well', 'so', 'what', 'hence', 'there', 'on', 'indeed', 'you', 'do', 'this',\n",
    "                'thus', 'here', 'him', 'say', 'thee', 'be', 'it','thou', 'to', 'come', 'go', 'away',\n",
    "                'he', 'more', 'one', 'see', 'speak', 'stay', 'that', 'then', 'up', 'yet', 'by', 'all',\n",
    "                'in', 'we', 'again', 'down', 'enough', 'for', 'have', 'her', 'himself', 'hold', 'how',\n",
    "                'no', 'i', 'now', 'me', 'mine', 'none', 'not', 'of', 'still', 'out', 'farewell', 'fellow'\n",
    "                'words', 'right', 'time', 'done', 'gentlemen', 'grace', 'sir', 'fellow', 'help', 'lady', \n",
    "                'myself', 'long', 'masters', 'name', 'queen', 'rest', 'sons', 'stand', 'tomorrow', 'tonight', 'words'\n",
    "                'together', 'too', 'will', 'a', 'o', 'am', 'another', 'answer', 'are', 'arm', 'bear', 'best', 'both'\n",
    "                'cause', 'comes', 'arms', 'art', 'back', 'both', 'cause', 'cousin', 'daughter', 'ever', 'fall',\n",
    "                'first', 'fly', 'further', 'his', 'pardon', 'much', 'madam', 'look', 'leave', 'last', 'leave',\n",
    "                'look', 'madam', 'much', 'pardon', 'sleep', 'she', 'shall', 'ill', 'is', 'know', 'upon', 'thyself',\n",
    "                'think', 'thine', 'thanks', 'tell', 'sister', 'return', 'prince', 'aside', 'wife', 'welcome',\n",
    "                'eye']\n",
    "mask_ban = ~np.isin(common_words, banned_words)\n",
    "\n",
    "preferred_word = ['lord', 'good', 'dead', 'love', 'true', 'faith', 'death', 'honour', \n",
    "                  'god', 'die', 'hell', 'heaven', 'joy', 'patience', 'nothing', 'shame',\n",
    "                  'fear', 'fortune', 'enemies']\n",
    "mask_inc = np.isin(common_words, preferred_word)\n",
    "\n",
    "\n",
    "common_words = common_words[mask_ban]\n",
    "max_words = 500\n",
    "common_words = common_words[:max_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comedy_2d = c_n.w_embedding.weight.detach().cpu().numpy() +  c_n.c_embedding.weight.detach().cpu().numpy() \n",
    "comedy_points = np.array([comedy_2d[common_dict[i]] for i in common_words])\n",
    "\n",
    "#used to help with graphing\n",
    "comedy_data = pd.DataFrame({\n",
    "    'x':comedy_points[:, 0],\n",
    "    'y':comedy_points[:, 1],\n",
    "    'genre':'comedy',\n",
    "    'word':common_words\n",
    "})\n",
    "\n",
    "tragedy_2d = t_n.w_embedding.weight.detach().cpu().numpy() +  t_n.c_embedding.weight.detach().cpu().numpy() \n",
    "tragedy_points = np.array([tragedy_2d[common_dict[i]] for i in common_words])\n",
    "\n",
    "tragedy_data = pd.DataFrame({\n",
    "    'x':tragedy_points[:, 0],\n",
    "    'y':tragedy_points[:, 1],\n",
    "    'genre':'tragedy',\n",
    "    'word':common_words\n",
    "})\n",
    "\n",
    "history_2d = h_n.w_embedding.weight.detach().cpu().numpy() +  h_n.c_embedding.weight.detach().cpu().numpy() \n",
    "history_points = np.array([history_2d[common_dict[i]] for i in common_words])\n",
    "\n",
    "history_data = pd.DataFrame({\n",
    "    'x':history_points[:, 0],\n",
    "    'y':history_points[:, 1],\n",
    "    'genre':'history',\n",
    "    'word':common_words\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(comedy_2d, \"comedy_embeddings.pt\")\n",
    "torch.save(tragedy_2d, \"tragedy_embeddings.pt\")\n",
    "torch.save(history_2d, \"history_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x28c1e3e60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "app = Dash(__name__)\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id=\"scatter-plot\"),\n",
    "    html.P(\"percentage of words shown\"),\n",
    "    dcc.Slider(\n",
    "        id='slider',\n",
    "        min=10, max=100, step=10,\n",
    "        value=10),\n",
    "    html.P(\"Select genres:\"),\n",
    "    dcc.Checklist(\n",
    "        id='checklist',\n",
    "        options=[\n",
    "            {'label': 'Comedy', 'value': 'comedy'},\n",
    "            {'label': 'Tragedy', 'value': 'tragedy'},\n",
    "            {'label': 'History', 'value': 'history'}\n",
    "        ],\n",
    "        value=['comedy', 'tragedy', 'history'],  \n",
    "    ),\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output(\"scatter-plot\", \"figure\"), \n",
    "    Input(\"slider\", \"value\"),\n",
    "    Input(\"checklist\", \"value\"))\n",
    "\n",
    "def update_plot(percentage, genres):\n",
    "    global comedy_data, tragedy_data, history_data\n",
    "    if genres == []:\n",
    "        return px.scatter()\n",
    "    max = int(comedy_data.shape[0] * percentage/100)\n",
    "    lim_comedy_data = comedy_data[:max]\n",
    "    lim_tragedy_data = tragedy_data[:max]\n",
    "    lim_history_data = history_data[:max]\n",
    "    history_data = pd.DataFrame({\n",
    "    'x':history_points[:, 0],\n",
    "    'y':history_points[:, 1],\n",
    "    'genre':'history',\n",
    "    'word':common_words\n",
    "})\n",
    "    data = pd.DataFrame({\n",
    "        'x':[0,0,0],\n",
    "        'y':[-100,-100,-100],\n",
    "        'genre':['comedy', 'tragedy', 'history'],\n",
    "    })\n",
    "    if 'comedy' in genres:\n",
    "        data = pd.concat([data,lim_comedy_data])\n",
    "    if 'tragedy' in genres:\n",
    "        data = pd.concat([data,lim_tragedy_data])\n",
    "    if 'history' in genres:\n",
    "        data = pd.concat([data,lim_history_data])\n",
    "    fig = px.scatter(data, x=\"x\", y=\"y\", color=\"word\", symbol='genre', hover_name='word')\n",
    "    fig.update_yaxes(range = [-5,5])\n",
    "    fig.update_xaxes(range = [-5,5])\n",
    "    return fig\n",
    "\n",
    "app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
